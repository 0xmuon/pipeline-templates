{
  "version": "0.1",
  "type": "container",
  "meta": {
    "trigger": "benchmark"
  },
  "ops": [
    {
      "type": "container/run",
      "id": "vllm",
      "args": {
        "entrypoint": [],
        "cmd": [
          "/bin/sh",
          "-c",
          "python3 -m vllm.entrypoints.openai.api_server --model Valdemardi/DeepSeek-R1-Distill-Llama-70B-AWQ --served-model-name R1-Llama-70B-AWQ --port 9000 --max-model-len 100000"
        ],
        "image": "docker.io/vllm/vllm-openai:latest",
        "gpu": true,
        "expose": 9000
      }
    }
  ]
} 