# vLLM API

![vLLM](https://avatars.githubusercontent.com/u/136984999?v=4)

A high-throughput and memory-efficient inference engine for Large Language Models.

Unleash the power of efficient LLM inference with Nosana! Effortlessly run your vLLM instance on high-performance GPU-backed nodes, ensuring optimal serving for your applications.

## Key Features
- PagedAttention technology
- Continuous batching
- OpenAI-compatible API
- Memory optimization
- GPU acceleration support

## Configuration
- Port: 8080
- GPU: Required
- Model: Qwen2.5-7B